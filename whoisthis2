###### Min Specs
- 4 cores
- 8-16 GB RAM

###### Base operating system
- For this course, CentOS 7 is used.

###### Shared resources
- Network
- CPU
- Storage
- RAM

###### Sensor - Multi-node
- Give Senor the most CPUs
- R840 Sensor
- R440 ESXI (ELK)


Storage = 2(Bavg x 86,400/8) Bavg = Link Speed
- PCAP =
- Meta = ~140:1
- Alert = ~1000:1

###### Note: 86,400 is the number of seconds in a day.

Stenographer takes packets from wire and writes to disk.
- Writes to two locations:
- /*/packets
- /*/index

By default, Steno has one thread which can take about 500 MiB per second (~500 Mb/s).

###### Multi-threading is possible.

- lscpu -e shows number of cores/threads available.

###### Quick ways to view traffic collection.
- tcpdump -i <interface name>
- watch ifstat

###### Three ring buffer options.
- Aids in passing of data to Suricata, Zeek, and Steno.
- Native to Rhel and CentOS.
- Increase efficiency by approximately 15%
1. Pf_Ring
2. Af_Packetv2 (Rhel 7)
- Tpacketv3 (Rhel 8+)
3. XDP (zero copy)


###### Logstash
- Data normalization - making all of your data conform to a specific data schema.  In Elastic, that's ECS (Elastic Common Schema).
- Data enrichment - adding data to the original logs to give more context.
- Logstash has three main components: Input, Filter, Output.
- For class, our Input is Kafka.  Filter is Data normalization and enrichment.  Output is Elasticsearch.

###### Kafka
- Guarantees delivery of data.
- Load balances.
- Buffers

----
## Producers (produce data and sends to Kafka cluster)
- Zeek
- Filebeat (sends data to Kafka)

## Brokers
#### - Kafka cluster itself
###### Topic (topic is a data source)
- Zeek
- Suricata
- FSF

## Consumers (subscribe to topics it requires)
- Logstash
- Input (workers)
- Filter
- Output

## Consumer Offset
----

###### Zookeeper
- Understands state of Kafha brookers (tracks cluster state).  
- Must be started before Kafka.  If it doesn't establish a quorum, Kafka will not come up.
- One Zookeeper (only one is necessary) instance can track multiple Kafka instances.
- yum install kafka librdkafka zookeeper (installs three items).
- librdkafka is the library that kafka uses.

###### Zeek
- Network protocol analyzer/parser.
- It is also a scripting language.
- Operates in a clustered state.
- Pin to CORES
- Will have a manager, logger, and workers.
- Each worker can handle ~250Mb/s. But mostly likely only ~200Mb/s will be obtained.  Single threaded, so these numbers are per CORE.
- Every single connection made goes into conn.log file
- If it sees a particular connection type, it will make a log.  For example: dns traffic goes into dns.log, etc.
- Specific traffic can go into multiple logs.  TLS traffic will go in conn.log, http.log, and ssl.log
- Zeek outputs to Kafka (stream)

###### Filebeat
- Inputs and outputs
- Inputs eve.json, rockout.log, and Outputs to Kafka.
- For filebeat backup, make a copy titled filebeat.yml.bak (filebeat won't recognize .bak files).

###### Suricata
- Writes to eve.json (file)

###### FSF
- Has two components: Client and Server
- Zeek tells Client to send files to Server
- Server has yara rules where is tests and scans the files.
- Yara rules outputs a json file.  scan_log is the defaut log; for class it writes to rockout.log (file)
- Contents of the json file get picked up by filebeat and shipped to kafka.
- Even if no yara rules are not defined, it will automatically record file hashes.

###### Note: Running cat on an executable can crash your terminal.

###### Elasticsearch
- Elasticsearch scales horizontally instead of vertically.
- Node is an instance of Elasticsearch.
- Cluster is a group of nodes.
- In clusters, you have master nodes.  Masters are like work place managers.  The rule is you don't want any more or any less than 3.
- Data nodes are the workers.  The only minimum requirement for data nodes depends on the amount of data being ingested.  For our kits, the instructor said there should be 5.  Data nodes do the indexing as well as the reading when a search is being performed.
- There is also a coordinating node.  They speed up search time.  It is the interface between Kibana and the nodes.  Installed by default.  It doesn't have to be used, but there is no reason not to.
- port 9200 is the ES API interface, 9300 is for inter node communication.
- FYSA: Every node is a coordinating node.

###### Kibana
- Give 4 COREs and 8G of RAM.
- Front end for querying Elasticsearch.
- 5601 is the default port for Kibana.
- Look into file-based authentication if you don't have LDAP set up.
###### Note: ss -lnt is a command to see active ports on our box.

###### Troubleshooting useful commands
- df -h checks disk space
- du -h will provide size of directories within directories
- Check permissions with ll
- /user/share/kafka/bin/kafka-console-consumer.sh verifies data is being loaded into kafka topics.  Run after zeek, kafka, suricata, filebeat, fsf
- run after fsf curl, http://192.168.2.20
- Check stenographer/packets to see if data is being written to the directory
- For logstash issues, cat /var/log/logstash/logstash-plain.log
- For Elasticsearch troubleshooting, curl localhost:9200/(underscore)cat/nodes and curl localhost:9200/(underscore)cat/indices
- ss -lnt to look at listening ports
- journalctl -xe provides more information on services
- journalctl -xeu elasticsearch.service for information on  specific service (Elasticsearch for this example).
- firewall-cmd --list-all shows all open ports on the firewall.
- ls /var/log is also useful.
- For troubleshoot, work right to left of the diagram.  So, start with Kibana, then ES, then Kafka, etc.

## Review
###### Stenographer
- sytemctl status stenograher
- /data/stenographer/packets
- /data/stenographer/index
- chown -R stenographer:stenographer stenographer/
- /etc/stenographer/config
- stenokeys.sh stenographer stenographer writes certs and CA to /etc/stenographer/certs
- port 1234

###### Zeek
- zeekctl status
- zeekctl start
- /etc/zeek for node.cfg, networks.cfg, and zeekctl.cfg (defines how Zeek will behave).
- /usr/share/zeek/site/local.zeek for scripts
- /data/zeek/logs where Zeek writes logs (current).
- /data/zeek/extract_files for file extraction
- /var/spool/zeek linked to current
- zcat for looking at zipped files.

###### FSF
- systemctl start fsf
- systemctl status fsf
- /data/fsf
- Two components (client, server)
- port 5800
- vi /opt/fsf/fsf-server/conf/config.py
- vi /opt/fsf/fsf-client/conf/config.py

###### Suricata
- /etc/suricata/suricata.yml
- /etc/suricata/rules
- /var/lib/suricata/suricata.rules where are actual rules are loaded
- /data/suricata/eve.json
- chown -R suricata:suricata suricata/ when in /data
- /etc/sysconfig/suricata
- enabled: no (for system collection)
- disable stats and fastlog
- af_packet = 99

###### Kafka
- ports 9092
- /data/kafka
- zookeeper manager - ports 2181, 2182, 2183
- /producers.properties
- /consumer.properties
- /etc/kafka/server.properties

###### Logstash
- default port 5044:beats
- /etc/logstash
- /etc/logstash/conf.d
- logstash.yml
- jvm.options (max is 8 for heap/ram)
- Running Pipelines are Input, Filter, and Output
- used for data normalization and enrichment
- adds geotagging and conforms data to ecs.
- Input is Kakfa, Output is ES.

###### Elasticsearch
- ports 9200 - API, 9300 - internal
- /etc/elasticsearch/elasticsearch.yml
- /etc/elasticsearch/jvm.options (max out: 31, or 50% of total RAM)
- 3 Masters
- 5 Data (workers)
- 1 CN (coordination)
- nodes types are data, ingest, machine learning, remotes, transform, master
- indices hold documents
- data (from biggest to smallest) index, document, shard
- shards consist of lucene indexes
- manage shards and replicas with index lifecycle management (ILM)
- /usr/lib/systemd/system/elasticsearch.service.d/override.conf to set MEMLOCK (this is done because we disable swap space).

###### Kibana
- port 5601
- /etc/kibana/kibana.yml for main configurations.  We modify server host -> routable IP.  Also modify elasticsearch.host (local machine in our case).
- start the service
- pull down mappings using script (./import-index-templates.sh).  This applies mappings to Elasticsearch.
- Get rid of replicas for green health data type.
- delete old indices -> mappings apply to all
- create index patterns: ecs-*, ecs-zeek-*, ecs-suricata-*, fsf-*

## Exercise
### Application of resources exercise
- Scenario is one server that requires capture 1GB/s
- Determine resources.
- 3 Master nodes, 1 data node, and Kibana recommended.
- Total # CORES
- Total # RAM
- From there, determine how much Zeek, Steno, etc get.
- Provide component breakdown.

### What our team found
- Steno - 1 CORE, 4g RAM
- Zeek - 3 CORE, 128g RAM
- Suri - 2 CORE, 8g RAM
- FSF - 2 CORE, 8g RAM
- Fileb - NEG, NEG 
- Kafka - 4 CORE, 8g RAM
- Logst - , 8g RAM
- ES - 4 CORE (ea), 16g RAM (ea)/ so think for 3 Masters and 1 Data node (6 CORE, 64g for data). These # were edited during debrief (so adjust CORE and RAM totals).
- Kib - 4 CORE, 8g RAM
- Zoo - 1 CORE, 4g RAM
- TOTAL - 19 CORE, 234g RAM (rounded up to 256g)
